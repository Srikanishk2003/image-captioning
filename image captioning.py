# -*- coding: utf-8 -*-
"""image_caption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DxSKSECSQs-ancdp9u_6QaxOoUTKueJC
"""

import tensorflow as tf
import re
from pickle import dump,load
import os
from os import listdir
from numpy import array,argmax
from IPython.display import Image,display

from keras.applications.vgg16 import VGG16,preprocess_input
from keras.utils import load_img,img_to_array
from keras.preprocessing.text import Tokenizer

from keras.models import Model
from keras.utils import pad_sequences
from keras.utils import to_categorical

from keras.layers import Input,Dense,LSTM,Embedding,Dropout
from keras.layers import add

from nltk.translate.bleu_score import corpus_bleu

!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip
!unzip Flickr8k_text.zip
!rm Flickr8k_text.zip

all_captions="/content/Flickr8k.token.txt"
train_captions="/content/Flickr_8k.trainImages.txt"
test_captions="/content/Flickr_8k.testImages.txt"

!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
!unzip Flickr8k_Dataset.zip
!rm Flickr8k_Dataset.zip

all_images ="/content/Flicker8k_Dataset"

file1 = open(all_captions,"r")
data = file1.read()
file1.close()

descr = dict()
for line in data.split("\n"):
  token = line.split()
  if len(token) < 2: #to skip blank lines
    continue
  image_id,image_descr = token[0],token[1:]
  image_id = image_id.split(".")[0]
  image_descr = "<start>" "+" " ".join(image_descr)+"<end>"

  if image_id not in descr:
    descr[image_id]= list()
  descr[image_id].append(image_descr)

for key,des, in descr.items():
  for i in range(len(des)):
    d = des[i].lower()
    d = re.sub(r"\w*[^a-z\s<>]+\w*","",d)
    des[i] = d
  descr[key] = des

vocab = set()

for k,v in descr.items():
  [vocab.update(l.split()) for l in v]
print(f"Original Vocabulary sizeis {len(vocab)}")

def load_txt(descr,filename):

  length = 0
  des = dict()
  file = open(filename,"r")
  keys = file.read()
  for k in keys.split("\n"):
    if len(k) < 2:
      continue
    image_id = k.split(".")[0]
    des[image_id]= descr.get(image_id)
    length+=1
  file.close()
  print(f"Number of Image ids: {length}")
  return des

train_descr = load_txt(descr,train_captions)
test_descr = load_txt(descr,test_captions)

"""Image preprocessing"""

all_images

listdir(all_images)

model = VGG16()

model.layers.pop() #remove the final layers

model = Model(inputs = model.inputs,outputs = model.layers[-1].output)

#learning features

features = dict()

for name in listdir(all_images):
  filename = all_images+"/" + name #for complete file path

  image = load_img(filename,target_size=(224,224)) #VGG16 IMAGE SIZE
  image = img_to_array(image)

  #reshaping to load into model

  image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))

  image = preprocess_input(image)

  feature = model.predict(image,verbose=0)
  image_id = name.split(".")[0]

  features[image_id] = feature

print(f"Extracted features are {len(features)}")

dump(features,open("features.dump","wb"))

def load_images_features(fn_features,fn_data):
  length=0
  feat = dict()
  file1=open(fn_data,"r")
  file2=open(fn_features,"rb" )
  f=load(file2)
  keys=file1.read()
  for k in keys.split("\n"):
    if len(k)<2:
      continue
    image_id = k.split(".")[0]
    feat[image_id] = f[image_id]
    length+=1
  file1.close()
  file2.close()
  print(f"Numberof instances are:{length}")
  return feat

train_Features = load_images_features("/content/features.dump",train_captions)
test_Features = load_images_features("/content/features.dump",test_captions)

def tokenizer(descr):
  d=[]
  for key in descr.keys():
     for i in descr[key]:
      d.append(i)
  tokenize = Tokenizer()
  tokenize.fit_on_texts(d)
  return tokenize

tokenize = tokenizer(train_descr)
vocab_size = len(tokenize.word_index)+1

d = []
for key in train_descr.keys():
  for i in train_descr[key]:
    d.append(i)
maxlen = max(len(i.split())for i in d)
print(f"Max length of caption in training data is {maxlen}")

def create_sequences(tokenizer,max_length,desc_list,photo,vocab_size):
  x1,x2,y = list(),list(),list()
  for desc in desc_list:
    seq = tokenizer.texts_to_sequences([desc])[0]
    for i in range(1,len(seq)):
      in_seq,out_seq = seq[:i],seq[i]
      in_seq = pad_sequences([in_seq],maxlen = max_length)[0]

      #encoding
      out_seq = to_categorical([out_seq],num_classes = vocab_size)[0]
      x1.append(photo)
      x2.append(in_seq)
      y.append(out_seq)

  return array(x1),array(x2),array(y)

def data_generator(descriptions,images,tokenizer,max_length,vocab_size):
  while 1:
    for key,desc_list in descriptions.items():
     image = images[key][0]
     in_img,in_seq,out_word = create_sequences(tokenizer,max_length,desc_list,image,vocab_size)
     yield [in_img,in_seq], out_word

def define_model(vocab_size,max_length):
  inputs1 = Input(shape=(1000,))
  fe1 = Dropout(0.5)(inputs1)
  fe2= Dense(256,activation="relu")(fe1)

  #sequence model
  inputs2 = Input(shape=(max_length,))
  se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)
  se2=Dropout(0.5)(se1)
  se3=LSTM(256)(se2)

  decoder1 = add([fe2,se3])
  decoder2= Dense(256,activation="relu")(decoder1)

  outputs = Dense(vocab_size,activation="softmax")(decoder2)

  model = Model(inputs = [inputs1,inputs2],outputs=outputs)

  model.compile(loss = "categorical_crossentropy", optimizer="adam")
  print(model.summary())
  return model

#tarining the model

model = define_model(vocab_size,maxlen)
epochs = 10
steps = len(train_descr)
for i in range (epochs):
  generator = data_generator(train_descr,train_Features,tokenize,maxlen,vocab_size)

  model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1)
  model.save("model_"+str(i)+".h5")

def word_for_id(integer,tokenizer):
  for word,index in tokenizer.word_index.items():
    if index == integer:
      return word
  return None

#generating description
def generate_desc(model,tokenizer,photo,max_length):
  in_text = "<start>"
  for i in range(max_length):
    sequence = tokenizer.texts_to_sequences[in_text][0]
    sequence = pad_sequences([sequence],maxlen=max_length)
    y_hat = model.predict([photo,sequence],verbose=0)

    y_hat = np.argmax(yhat)

    word = word_for_id(y_hat,tokenizer)

    if word is None:
      break
    in_text += " " + word
    if word =="end":
      break
    return in_text

def evaluate_model(model,descriptions,photos,tokenizer,max_length):
  actual,predicted = list(),list()
  for key,desc_list in descriptions.items():
    yhat = generate_desc(model,tokenizer,photos[key],max_length)

    references = [d.split() for d in desc_list]
    actual.append(references)
    predicted.append(yhat.split())
    print("BLEU-1:%f" %corpus_bleu(actual,predicted,weights=(1,0,0,0,0)))

evaluate_model(model,test_descr,test_features,tokenize,maxlen)